---
layout: default
title: Status
---

## Project Summary
This project teaches an agent to explore and survive in a contained but hostile environment. It trains the agent using multiple local search algorithms, and then compares the performance of each algorithm. Currently, both genetic and hill-climbing algorithms have been implemented, and tested in a simplified version of the final environment. This environment contains a single Zombie that the agent must avoid, as well as two diamonds that the agent can collect to maximize its score. The agent's score also improves the longer the agent stays alive.

The goals and approach for the project have remained largely unchanged since the proposal. Only a few implementation details have changed (e.g., score improves by collecting diamonds, not experience points). Several steps still remain until the project is complete. More algorithms will be implemented, and will be tested in an environment sufficiently complex that the differences between their performance are readily apparent. So far, however, the project provides a meaningful comparison between the two local search algorithms already implemented.

## Approach
Currently, the program uses two local search algorithms to navigate the environment. Each algorithm uses heuristic functions to decide which movement the agent will take. So far, the algorithms employ the following heuristic functions:

* `random_direction`: the agent moves in a random direction
* `away_from_enemy`: the agent moves away from the nearest zombie
* `towards_item`: the agent moves towards the nearest diamond

Each algorithm decides which heuristic function to use, and then returns the movement action generated by the heuristic. The agent then takes that action. Once the mission is complete, the agent's score is evaluated. The score is a function of the number of diamonds that the agent has collected, and the amount of time that the agent stayed alive. Specifically, the score is calculated as:

$$ score = diamonds \cdot 50 + time alive in seconds $$

Missions end when the agent has died, or automatically after 30 seconds. The score is then fed to the algorithm, which considers the score and then updates its heuristic selection function appropriately. Each algorithm has the following three functions defined in its interface:

* `__init__:` the algorithm's constructor, which takes in a list of heuristic functions that it can use
* `set_score:` informs the algorithm of the score it achieved
* `get_action`: returns the next move that the agent should make

So far, we have implemented a genetic algorithm and a hill-climbing algorithm. They work as follows:

#### Genetic Algorithm
The genetic algorithm creates a "generation" of strings of heuristic functions. Each generation contains 20 strings. The length of the strings is determined randomly by a normal distribution with $$\mu = 5$$ and $$\sigma = 1$$. Every mission, the agent relies on the heuristic strings to determine where to move. For each move, it looks to the next heuristic in the string, and takes the move generated by that heuristic. If the agent reaches the end of the string, it starts over at the beginning of the string.

After each mission, the algorithm is updated with the score that the agent received. When the agent has used all strings in a generation, the algorithm takes the top 5 most high-scoring strings, and generates 20 new strings. It generates the new strings by randomly selecting from the most high-scoring strings, and then combining them at a randomly chosen crossover point. It then repeats this process with the new generation, ideally attaining higher scores each time.

#### Hill-Climbing Algorithm
The hill-climbing algorithm operates on a list of heuristics with associated weights. Each time the agent needs to make a move, the algorithm selects from the available heuristics with a probability proportional to the heuristic's weight. It then returns the move generated by that heuristic.

Before every mission, the hill-climbing algorithm increases the weight of one of the heuristics. If, after the mission, this resulted in an increase in score, then it continues to increase that heuristic's weight. If instead the score decreases, the algorithm undoes its previous increase and instead selects another heuristic whose weight it will increase. Over time, this should lead to a heuristic preference that results in a higher score.

## Evaluation
*Work in progress*

## Remaining Goals and Challenges
The algorithms implemented so far have shown themselves up to the task of teaching the agent to navigate this simple environment. However, we would like to test these algorithms in a tougher environment, with more diamonds to collect, and potentially more enemies. We believe that a more difficult environment will provide a more interesting test for the algorithms. Furthermore, we want to compare more algorithms, so that we have a better idea of the strengths and weaknesses of each.

No challenges remaining before the final report is due should be especially difficult to overcome. The program is currently designed such that additional algorithms can be coded and used with only trivial change to existing code. Any remaining difficulty therefore lies primarily in the difficulty of coding the algorithms themselves. We hope to have enough to compare that, if some do not perform well, this will not hurt the overall quality of our comparative data.
